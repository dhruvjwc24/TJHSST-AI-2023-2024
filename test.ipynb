{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import gym # for environment\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam # adaptive momentum \n",
    "import random\n",
    "\n",
    "EPISODES = 1000\n",
    "\n",
    "class DQLAgent(): \n",
    "    \n",
    "    def __init__(self, env):\n",
    "        # parameters and hyperparameters\n",
    "        \n",
    "        # this part is for neural network or build_model()\n",
    "        self.state_size = env.observation_space.shape[0] # this is for input of neural network node size\n",
    "        self.action_size = env.action_space.n # this is for out of neural network node size\n",
    "        self.memory = deque(maxlen = 1000) # a list with 1000 memory, if it becomes full first inputs will be deleted\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        - implement a policy (epsilon-greedy recommended)\n",
    "        - add a model that selects actions (DQN = deep Q learning)\n",
    "        - recommended: implement experience replay for training\n",
    "        \"\"\"    \n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        # storage\n",
    "        self.memory.append((state, action, reward, next_state, done)) # (state, action, reward, next_state, done) tuple\n",
    "    \n",
    "    def act(self, state):\n",
    "        # acting, exploit or explore\n",
    "        if random.uniform(0,1) <= self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            act_values = self.model.predict(state) # predict Q values\n",
    "            return np.argmax(act_values[0]) # return the index of the max Q value\n",
    "            \n",
    "    \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # initialize gym environment and agent\n",
    "    env = gym.make('CartPole-v1')\n",
    "    agent = DQLAgent(env)\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    batch_size = 32\n",
    "\n",
    "    rolling_rewards = deque(maxlen=100)\n",
    "    avg_reward = 0\n",
    "    for e in range(EPISODES):\n",
    "        \n",
    "        # initialize environment\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state[0], [1, state_size])\n",
    "        for time in range(500):\n",
    "            ep_reward =0\n",
    "            env.render()\n",
    "            action = agent.act(state)\n",
    "            # print(env.step(action))\n",
    "            next_state, reward, done, _, _= env.step(action)\n",
    "            # print(\"next_state: \", next_state)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            reward = reward if not done else -10\n",
    "            ep_reward += reward\n",
    "            agent.memorize(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}, e: {}\".format(e, EPISODES, time, agent.epsilon))\n",
    "                break\n",
    "\n",
    "        rolling_rewards.append(ep_reward)\n",
    "        avg_reward = sum(rolling_rewards)/100\n",
    "        print(\"Average Reward:\", avg_reward)\n",
    "        if avg_reward >= 195.0:\n",
    "            print(\"Solved!\")\n",
    "            print(avg_reward)\n",
    "            agent.save(\"./save/cartpole-dqn.h5\")\n",
    "            exit(0)\n",
    "\n",
    "    print(\"Not solved :(\")\n",
    "    print(avg_reward)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
